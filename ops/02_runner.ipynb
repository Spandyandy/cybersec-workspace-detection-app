{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "786ee807-26d8-49db-95c2-15d697879dee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install geoip2 netaddr\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4234b5e0-bc6b-493e-8dd5-d6c2f75005d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import builtins\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "common = dbutils.import_notebook(\"lib.common\")  # repo의 /lib/common.py\n",
    "builtins.detect = common.detect                # @detect 데코레이터를 룰 노트북이 찾게 함\n",
    "builtins.Output = common.Output\n",
    "builtins.dbutils = dbutils\n",
    "builtins.spark = spark\n",
    "builtins.F = F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1879314d-3087-4b94-8389-d2e1b1a918b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 룰 노트북들이 import 시점에 dbutils.widgets.get(\"earliest\"/\"latest\")를 호출하므로\n",
    "# Runner에서 위젯을 먼저 만들어둔다.\n",
    "\n",
    "for k in [\"earliest\", \"latest\"]:\n",
    "    try:\n",
    "        dbutils.widgets.remove(k)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "dbutils.widgets.text(\"earliest\", \"\")\n",
    "dbutils.widgets.text(\"latest\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b29e8978-76ff-4fa9-985c-8ca23449797d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "nb_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "repo_ws_root = \"/\".join(nb_path.split(\"/\")[:4])          # /Repos/<user>/<repo>\n",
    "repo_fs_root = f\"/Workspace{repo_ws_root}\"               # /Workspace/Repos/<user>/<repo>\n",
    "materialized_fs_root = f\"{repo_fs_root}/materialized_py\" # /Workspace/Repos/<user>/<repo>/materialized_py\n",
    "\n",
    "if materialized_fs_root not in sys.path:\n",
    "    sys.path.insert(0, materialized_fs_root)\n",
    "\n",
    "print(\"added_to_sys.path =\", materialized_fs_root)\n",
    "print(\"sys.path[0:3] =\", sys.path[0:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86dc3875-07b8-497b-a1cf-a4becd363dab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks 위젯(파라미터)\n",
    "dbutils.widgets.text(\"rule_group\", \"binary\")      # binary | behavioral\n",
    "dbutils.widgets.text(\"rule_ids\", \"\")              # 옵션: 콤마로 rule_id 지정 (비우면 전체)\n",
    "\n",
    "rule_group = dbutils.widgets.get(\"rule_group\").strip()\n",
    "rule_ids_raw = dbutils.widgets.get(\"rule_ids\").strip()\n",
    "rule_ids = [x.strip() for x in rule_ids_raw.split(\",\") if x.strip()] if rule_ids_raw else []\n",
    "\n",
    "# 룰 로드 (네가 쓰는 스키마: sandbox.audit_poc)\n",
    "query = f\"\"\"\n",
    "SELECT rule_id, rule_group, module_path, callable_name, lookback_minutes\n",
    "FROM sandbox.audit_poc.rule_registry\n",
    "WHERE enabled = true\n",
    "  AND rule_group = '{rule_group}'\n",
    "\"\"\"\n",
    "\n",
    "if rule_ids:\n",
    "    ids = \",\".join([f\"'{x}'\" for x in rule_ids])\n",
    "    query += f\" AND rule_id IN ({ids})\"\n",
    "\n",
    "rules_df = spark.sql(query)\n",
    "\n",
    "display(rules_df)\n",
    "print(\"loaded_rules_count =\", rules_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e834fbf-2f2c-42fb-a0fa-cd3e3e213ad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, expr, coalesce\n",
    "\n",
    "# checkpoint 조인해서 window 계산\n",
    "# earliest = last_success_end_ts가 있으면 그 값, 없으면 now - lookback_minutes\n",
    "# latest   = now\n",
    "windows_df = (\n",
    "    rules_df.alias(\"r\")\n",
    "    .join(\n",
    "        spark.table(\"sandbox.audit_poc.rule_checkpoint\").alias(\"c\"),\n",
    "        on=\"rule_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"r.rule_id\"),\n",
    "        col(\"r.rule_group\"),\n",
    "        col(\"r.module_path\"),\n",
    "        col(\"r.callable_name\"),\n",
    "        col(\"r.lookback_minutes\"),\n",
    "        coalesce(\n",
    "            col(\"c.last_success_end_ts\"),\n",
    "            expr(\"current_timestamp() - INTERVAL 1 MINUTE * lookback_minutes\")\n",
    "        ).alias(\"window_start_ts\"),\n",
    "        current_timestamp().alias(\"window_end_ts\")\n",
    "    )\n",
    ")\n",
    "\n",
    "display(windows_df.select(\"rule_id\",\"window_start_ts\",\"window_end_ts\",\"lookback_minutes\").orderBy(\"rule_id\"))\n",
    "print(\"windows_count =\", windows_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "095870ba-fc8d-4411-a26d-0ef03d945a9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "nb_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "repo_ws_root = \"/\".join(nb_path.split(\"/\")[:4])  # /Repos/<user>/<repo>\n",
    "\n",
    "updates = []\n",
    "for rg in [\"binary\", \"behavioral\"]:\n",
    "    folder_ws = f\"{repo_ws_root}/base/detections/{rg}\"\n",
    "    folder_fs = f\"/Workspace{folder_ws}\"\n",
    "\n",
    "    for fname in sorted(os.listdir(folder_fs)):\n",
    "        if not fname.endswith(\".py\") or fname.startswith(\"_\"):\n",
    "            continue\n",
    "        rule_id = fname[:-3]\n",
    "        notebook_path = f\"{folder_ws}/{fname}\"\n",
    "        updates.append((rule_id, notebook_path))\n",
    "\n",
    "df_upd = spark.createDataFrame(updates, \"rule_id string, notebook_path string\")\n",
    "df_upd.createOrReplaceTempView(\"upd\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO sandbox.audit_poc.rule_registry t\n",
    "USING upd s\n",
    "ON t.rule_id = s.rule_id\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "  t.notebook_path = s.notebook_path,\n",
    "  t.run_mode = 'NOTEBOOK',\n",
    "  t.updated_at = current_timestamp()\n",
    "\"\"\")\n",
    "\n",
    "display(spark.sql(\"SELECT rule_group, COUNT(*) cnt, COUNT(notebook_path) nb_cnt FROM sandbox.audit_poc.rule_registry GROUP BY rule_group\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "58f62eda-4cb6-464e-875b-92654c399145",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 룰 1개 테스트: dbutils.import_notebook 방식\n",
    "\n",
    "one = windows_df.orderBy(\"rule_id\").limit(1).collect()[0]\n",
    "rule_id = one[\"rule_id\"]\n",
    "start_ts = one[\"window_start_ts\"]\n",
    "end_ts = one[\"window_end_ts\"]\n",
    "\n",
    "r = spark.sql(f\"\"\"\n",
    "SELECT module_path, callable_name\n",
    "FROM sandbox.audit_poc.rule_registry\n",
    "WHERE rule_id = '{rule_id}'\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "module_path = r[\"module_path\"]      # 예: base.detections.binary.attempted_logon_from_denied_ip\n",
    "callable_name = r[\"callable_name\"]  # 예: attempted_logon_from_denied_ip\n",
    "\n",
    "print(\"TEST RULE:\", rule_id, module_path, callable_name)\n",
    "print(\"WINDOW:\", start_ts, \"->\", end_ts)\n",
    "\n",
    "mod = dbutils.import_notebook(module_path)\n",
    "fn = getattr(mod, callable_name)\n",
    "\n",
    "result_df = fn(earliest=str(start_ts), latest=str(end_ts))\n",
    "\n",
    "print(\"result_count =\", result_df.count())\n",
    "display(result_df.limit(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "084852a9-bd99-4857-ba7f-df8838aa3cdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit, sha2, concat_ws\n",
    "\n",
    "target_table = f\"sandbox.audit_poc.findings_{rule_id}\"\n",
    "\n",
    "# 공통 컬럼 붙이기 + dedupe_key 만들기(빈 DF여도 스키마 맞춤용)\n",
    "out_df = (\n",
    "    result_df\n",
    "    .withColumn(\"observed_at\", current_timestamp())\n",
    "    .withColumn(\"rule_id\", lit(rule_id))\n",
    "    .withColumn(\"run_id\", lit(\"manual_test\"))\n",
    "    .withColumn(\"window_start_ts\", lit(start_ts))\n",
    "    .withColumn(\"window_end_ts\", lit(end_ts))\n",
    ")\n",
    "\n",
    "# 결과에 명확한 키 컬럼이 없으니, 전체 row를 문자열로 해시해서 dedupe_key로 사용\n",
    "out_df = out_df.withColumn(\"dedupe_key\", sha2(concat_ws(\"||\", *[out_df[c].cast(\"string\") for c in out_df.columns]), 256))\n",
    "\n",
    "# 테이블 생성/적재 (0건이면 스키마만 생성되도록)\n",
    "(out_df.write\n",
    " .format(\"delta\")\n",
    " .mode(\"append\")\n",
    " .option(\"mergeSchema\", \"true\")\n",
    " .saveAsTable(target_table)\n",
    ")\n",
    "\n",
    "print(\"saved_to:\", target_table)\n",
    "display(spark.sql(f\"SELECT COUNT(*) AS cnt FROM {target_table}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d83cfe8f-09e4-4bf0-b5be-69235423ec80",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"run_id\":204},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770667874502}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time, importlib\n",
    "from pyspark.sql.functions import current_timestamp, lit, sha2, concat_ws\n",
    "\n",
    "run_id = f\"manual_binary_{int(time.time())}\"\n",
    "\n",
    "# binary 룰들만\n",
    "binary_rules = windows_df.filter(\"rule_group = 'binary'\").collect()\n",
    "\n",
    "results = []\n",
    "\n",
    "for r in binary_rules:\n",
    "    rule_id = r[\"rule_id\"]\n",
    "    start_ts = r[\"window_start_ts\"]\n",
    "    end_ts = r[\"window_end_ts\"]\n",
    "\n",
    "    # registry에서 module_path/callable_name 재조회(최신값 보장)\n",
    "    meta = spark.sql(f\"\"\"\n",
    "    SELECT module_path, callable_name\n",
    "    FROM sandbox.audit_poc.rule_registry\n",
    "    WHERE rule_id = '{rule_id}' AND enabled = true\n",
    "    \"\"\").collect()\n",
    "    if not meta:\n",
    "        continue\n",
    "    module_path, callable_name = meta[0][\"module_path\"], meta[0][\"callable_name\"]\n",
    "\n",
    "    t0 = time.time()\n",
    "    status = \"SUCCESS\"\n",
    "    err = None\n",
    "    row_count = None\n",
    "\n",
    "    try:\n",
    "        mod = dbutils.import_notebook(module_path)\n",
    "        fn = getattr(mod, callable_name)\n",
    "        df = fn(earliest=str(start_ts), latest=str(end_ts))\n",
    "\n",
    "        row_count = df.count()\n",
    "\n",
    "        out_df = (\n",
    "            df\n",
    "            .withColumn(\"observed_at\", current_timestamp())\n",
    "            .withColumn(\"rule_id\", lit(rule_id))\n",
    "            .withColumn(\"run_id\", lit(run_id))\n",
    "            .withColumn(\"window_start_ts\", lit(start_ts))\n",
    "            .withColumn(\"window_end_ts\", lit(end_ts))\n",
    "        )\n",
    "\n",
    "        # dedupe_key: row 전체 해시\n",
    "        out_df = out_df.withColumn(\n",
    "            \"dedupe_key\",\n",
    "            sha2(concat_ws(\"||\", *[out_df[c].cast(\"string\") for c in out_df.columns]), 256)\n",
    "        )\n",
    "\n",
    "        target_table = f\"sandbox.audit_poc.findings_{rule_id}\"\n",
    "        (out_df.write.format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .saveAsTable(target_table)\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        status = \"FAILED\"\n",
    "        err = str(e)[:4000]\n",
    "\n",
    "    duration_ms = int((time.time() - t0) * 1000)\n",
    "    results.append((run_id, rule_id, str(start_ts), str(end_ts), status, row_count, duration_ms, err))\n",
    "\n",
    "# 요약 출력\n",
    "summary_df = spark.createDataFrame(\n",
    "    results,\n",
    "    \"run_id string, rule_id string, window_start string, window_end string, status string, row_count long, duration_ms long, error string\"\n",
    ")\n",
    "display(summary_df.orderBy(\"status\", \"rule_id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18961526-0564-4c89-bb49-16be62090728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time, importlib\n",
    "from pyspark.sql.functions import current_timestamp, lit, sha2, concat_ws\n",
    "\n",
    "run_id = f\"manual_behavioral_{int(time.time())}\"\n",
    "\n",
    "# behavioral 룰들만\n",
    "behavioral_rules = windows_df.filter(\"rule_group = 'behavioral'\").collect()\n",
    "\n",
    "results = []\n",
    "\n",
    "for r in behavioral_rules:\n",
    "    rule_id = r[\"rule_id\"]\n",
    "    start_ts = r[\"window_start_ts\"]\n",
    "    end_ts = r[\"window_end_ts\"]\n",
    "\n",
    "    # registry에서 module_path/callable_name 재조회(최신값 보장)\n",
    "    meta = spark.sql(f\"\"\"\n",
    "    SELECT module_path, callable_name\n",
    "    FROM sandbox.audit_poc.rule_registry\n",
    "    WHERE rule_id = '{rule_id}' AND enabled = true\n",
    "    \"\"\").collect()\n",
    "    if not meta:\n",
    "        continue\n",
    "    module_path, callable_name = meta[0][\"module_path\"], meta[0][\"callable_name\"]\n",
    "\n",
    "    t0 = time.time()\n",
    "    status = \"SUCCESS\"\n",
    "    err = None\n",
    "    row_count = None\n",
    "\n",
    "    try:\n",
    "        mod = dbutils.import_notebook(module_path)\n",
    "        fn = getattr(mod, callable_name)\n",
    "        df = fn(earliest=str(start_ts), latest=str(end_ts))\n",
    "\n",
    "        row_count = df.count()\n",
    "\n",
    "        out_df = (\n",
    "            df\n",
    "            .withColumn(\"observed_at\", current_timestamp())\n",
    "            .withColumn(\"rule_id\", lit(rule_id))\n",
    "            .withColumn(\"run_id\", lit(run_id))\n",
    "            .withColumn(\"window_start_ts\", lit(start_ts))\n",
    "            .withColumn(\"window_end_ts\", lit(end_ts))\n",
    "        )\n",
    "\n",
    "        # dedupe_key: row 전체 해시\n",
    "        out_df = out_df.withColumn(\n",
    "            \"dedupe_key\",\n",
    "            sha2(concat_ws(\"||\", *[out_df[c].cast(\"string\") for c in out_df.columns]), 256)\n",
    "        )\n",
    "\n",
    "        target_table = f\"sandbox.audit_poc.findings_{rule_id}\"\n",
    "        (out_df.write.format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .saveAsTable(target_table)\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        status = \"FAILED\"\n",
    "        err = str(e)[:4000]\n",
    "\n",
    "    duration_ms = int((time.time() - t0) * 1000)\n",
    "    results.append((run_id, rule_id, str(start_ts), str(end_ts), status, row_count, duration_ms, err))\n",
    "\n",
    "# 요약 출력\n",
    "summary_df = spark.createDataFrame(\n",
    "    results,\n",
    "    \"run_id string, rule_id string, window_start string, window_end string, status string, row_count long, duration_ms long, error string\"\n",
    ")\n",
    "display(summary_df.orderBy(\"status\", \"rule_id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c330c76-30e1-4ce5-a782-debeb0de4b0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 통합 테이블 적재함수\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "UNIFIED_TBL = \"sandbox.audit_poc.findings_unified\"\n",
    "DEFAULT_SEVERITY = \"medium\"\n",
    "\n",
    "# rule_registry에 severity 컬럼이 있으면 읽고, 없으면 전부 기본값 사용\n",
    "_registry_cols = set(spark.table(REGISTRY_TBL).columns)\n",
    "_has_severity = \"severity\" in _registry_cols\n",
    "\n",
    "if _has_severity:\n",
    "    _sev_rows = spark.table(REGISTRY_TBL).select(\"rule_id\", \"severity\").collect()\n",
    "    SEVERITY_MAP = {r[\"rule_id\"]: (r[\"severity\"] or DEFAULT_SEVERITY) for r in _sev_rows}\n",
    "else:\n",
    "    SEVERITY_MAP = {}\n",
    "\n",
    "def _resolve_event_ts_col(df):\n",
    "    for c in [\"EVENT_TIME\", \"event_time\", \"event_ts\", \"EVENT_TS\", \"timestamp\", \"time\"]:\n",
    "        if c in df.columns:\n",
    "            return F.col(c).cast(\"timestamp\")\n",
    "    return F.current_timestamp()\n",
    "\n",
    "def _write_unified(rule_id: str, df, run_id: str, start_ts, end_ts) -> None:\n",
    "    event_ts_expr = _resolve_event_ts_col(df)\n",
    "    payload_json_expr = F.to_json(F.struct(*[F.col(c) for c in df.columns])) if df.columns else F.lit(\"{}\")\n",
    "\n",
    "    severity = SEVERITY_MAP.get(rule_id, DEFAULT_SEVERITY)\n",
    "\n",
    "    unified_df = (\n",
    "        df.select(\n",
    "            event_ts_expr.alias(\"event_ts\"),\n",
    "            F.to_date(event_ts_expr).alias(\"event_date\"),\n",
    "            F.lit(rule_id).alias(\"rule_id\"),\n",
    "            F.lit(severity).alias(\"severity\"),\n",
    "            F.lit(run_id).alias(\"run_id\"),\n",
    "            F.lit(start_ts).alias(\"window_start_ts\"),\n",
    "            F.lit(end_ts).alias(\"window_end_ts\"),\n",
    "            F.current_timestamp().alias(\"observed_at\"),\n",
    "            payload_json_expr.alias(\"payload_json\"),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"dedupe_key\",\n",
    "            F.sha2(\n",
    "                F.concat_ws(\n",
    "                    \"||\",\n",
    "                    F.col(\"rule_id\"),\n",
    "                    F.coalesce(F.col(\"event_ts\").cast(\"string\"), F.lit(\"\")),\n",
    "                    F.col(\"payload_json\"),\n",
    "                ),\n",
    "                256,\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    (unified_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(UNIFIED_TBL)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70e1ec4f-2b09-45cb-9bf1-025c1f717ae9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_runner",
   "widgets": {
    "earliest": {
     "currentValue": "",
     "nuid": "52f96e23-d06f-444a-9722-51b2628e48a0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "earliest",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "earliest",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "latest": {
     "currentValue": "",
     "nuid": "197cbdcd-7f92-426f-a04a-b8994d28d20c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "latest",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "latest",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "rule_group": {
     "currentValue": "behavioral",
     "nuid": "a6625603-37ed-43f9-8847-9b83922d86d8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "binary",
      "label": null,
      "name": "rule_group",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "binary",
      "label": null,
      "name": "rule_group",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "rule_ids": {
     "currentValue": "",
     "nuid": "f3743364-6788-43bb-bb3c-fc6f4f94600f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "rule_ids",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "rule_ids",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
