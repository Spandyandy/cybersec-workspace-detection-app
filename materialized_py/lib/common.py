# MAGIC %pip install geoip2 netaddr --quiet


from pyspark import SparkFiles, SparkContext
import geoip2.errors
from geoip2 import database
import os
import re
import shutil
import tempfile
from typing import Dict, Any, Union, Optional  
from abc import abstractmethod, ABC

from netaddr import IPAddress, AddrFormatError
from pyspark.sql import Column
import pyspark.sql.functions as F
from pyspark.sql import DataFrame
import pandas as pd

def is_public_ip(s: str) -> Optional[bool]:
    """
    Check if the given string is IP and belongs to public IPs (not loopback, private, etc.).
    Primarily it's used to avoid lookup of MaxMind database for data that doesn't exist
    :param s: string to check
    :return: true if string is global IP address
    """
    if s:
        try:
            return IPAddress(s).is_global()
        except (ValueError, AddrFormatError):
            pass

    return None

def generate_ip_range_condition(cl: Column, tmpl: str, range_start, range_end) -> Column:
    """
    Generates condition that checks if IP address is in the specified range
    :param cl: column with IP address
    :param tmpl: template for IP address range
    :param range_start: start of the range
    :param range_end: end of the range
    :return: condition
    """
    cond = None
    for i in range(range_start, range_end + 1):
        new_cond = F.startswith(cl, F.lit(tmpl.format(i)))
        if cond is None:
            cond = new_cond
        else:
            cond = cond | new_cond

    return cond

class EnrichmentBase(ABC):
    """Base class for data enrichment.  It could be used for data enrichment using functions, or
    for example, by joining with another dataframe
    """

    __not_found_marker__ = "NotFound"

    def __init__(self, name: str):
        self._name = name

    @abstractmethod
    def enrich(self, df: DataFrame) -> DataFrame:
        """Performs enrichment of the given DataFrame

        :param df: DataFrame to enrich
        :return: enriched DataFrame
        """
        pass

class ColumnEnrichment(EnrichmentBase):
    """Class that enriches DataFrame with a single column
    """
    def __init__(self, name: str, alias: Optional[str] = None):
        super().__init__(name)
        self._alias = alias

    @abstractmethod
    def get_column(self, src: Optional[str] = None, alias: Optional[str] = None) -> Column:
        """Implementation need to provide this method by returning a Column object that
        performs enrichment of the data

        :return:
        """
        pass

    def enrich(self, df: DataFrame) -> DataFrame:
        """Enriches DataFrame by adding a new column with data generated by specified function.
        The main drawback of this implementation is that it won't expand data if column is struct
        :param df: DataFrame to enrich
        :return: new DataFrame with added column

        """
        cl = self.get_column()
        if self._alias:
            cl = cl.alias(self._alias)
        return df.select("*", cl)

class PandasFunctionEnrichmentBase(ColumnEnrichment):
    """Base class for all enrichment implementations based on the Pandas UDFs
    """
    def __init__(self, name: str, src_column_name_or_expr: str, dest_column_name: str):
        """
        Initializer
        :param name: name of implementation
        :param src_column_name_or_expr: name of the source column or SQL expression
        :param dest_column_name: name of the column in which data will be stored
        """
        super().__init__(name)
        self._src_column_name_or_expr = src_column_name_or_expr
        self._dest_column_name = dest_column_name

    @abstractmethod
    def create_pandas_udf_function(self):
        """Implementations need to override this function with a function that will return
         UDF/Pandas UDF that will perform actual enrichment
        """
        pass

    def get_column(self, src: Optional[str] = None, alias: Optional[str] = None) -> Column:
        if not src:
            src = self._src_column_name_or_expr
        if not alias:
            alias = self._dest_column_name
        return self.create_pandas_udf_function()(F.expr(src)).alias(alias)

class MaxMindEnrichmentBase(PandasFunctionEnrichmentBase):
    """Base class for all enrichment implementations based on the MaxMind databases
    """

    def __init__(self, name: str, db_file: str, ip_column_name_or_expr: str, dest_column_name):
        """
        Initializer
        :param name: implementation name
        :param db_file: path to file with MaxMind database (it could be a local file, file on WSFS or Volumes)
        :param ip_column_name_or_expr: name of the column with IP information or SQL expression
        :param dest_column_name: name of the column in which data will be stored
        """
        super().__init__(name, ip_column_name_or_expr, dest_column_name)
        idx = db_file.rfind("/")
        if idx == -1:
            raise Exception(f"Please specify correct file name! got '{db_file}'")

        if db_file.startswith("dbfs:/"):
            self._dbfs_file_name = "/dbfs" + re.sub(r"^dbfs:(.*)$", r"\1", db_file)
        else:
            self._dbfs_file_name = db_file

        self._file_name = db_file[(idx + 1):]

        self._ip_column_name_or_expr = ip_column_name_or_expr
        self._dest_column_name = dest_column_name
        self.__local_tmp_directory__ = tempfile.gettempdir()

    def _copy_db_file(self, local_path):
        fd, tmp_name = tempfile.mkstemp(dir=self.__local_tmp_directory__)
        os.close(fd)
        shutil.copy2(self._dbfs_file_name, tmp_name)
        os.rename(tmp_name, local_path)

    def _get_database(self):
        """
        Returns a file name to read database from.
        :return: database file name
        """
        db_name = self._dbfs_file_name
        try:
            local_path = os.path.join(self.__local_tmp_directory__, self._file_name)
            if not os.path.exists(local_path):
                # print("No local copy found, copying")
                os.makedirs(self.__local_tmp_directory__, exist_ok=True)
                self._copy_db_file(local_path)
            else:
                # print("Local copy is older than remote copy, copying new version")
                lstat = os.stat(local_path)
                rstat = os.stat(self._dbfs_file_name)
                if rstat.st_mtime > lstat.st_mtime:
                    self._copy_db_file(local_path)
                db_name = local_path
        except OSError as e:
            print(f"OS error occurred, using remote file directly: {e}")

        return database.Reader(db_name)

    @abstractmethod
    def create_pandas_udf_function(self):
        """Implementations need to override this function with a function that will return
         UDF/Pandas UDF that will perform actual enrichment
        """
        pass

    @abstractmethod
    def __type__for_null__(self) -> str:
        pass

    def get_column(self, src: Optional[str] = None, alias: Optional[str] = None) -> Column:
        if not src:
            src = self._src_column_name_or_expr
        if not alias:
            alias = self._dest_column_name

        cl = F.expr(src)
        # TODO: add more checks for private IP addresses (RFC 3330)
        skip_cond = (cl.isNull() | (cl == "") | (~F.contains(cl, F.lit(".")) & ~F.contains(cl, F.lit(":"))) |
                     F.startswith(cl, F.lit("127.")) | F.startswith(cl, F.lit("192.168.")) |
                     F.startswith(cl, F.lit("10.")) | F.startswith(cl, F.lit("169.254.")) |
                     generate_ip_range_condition(cl, "172.{}.)", 16, 31))
        new_cl = (F.when(skip_cond, F.lit(None).cast(self.__type__for_null__())).otherwise(
            self.create_pandas_udf_function()(cl))).alias(alias)

        return new_cl


class GeoIPEnrichment(MaxMindEnrichmentBase):
    """Class that enriches DataFrame with GeoIP data
    """
    EMPTY_RECORD = {'city': None, 'country': None, 'country_code': None,
                    'latitude': None, 'longitude': None, 'accuracy_radius': None}

    def __init__(self, db_file: str, ip_column_name_or_expr: str, dest_column_name: str = "geo"):
        super().__init__("GeoIP", db_file, ip_column_name_or_expr, dest_column_name)
        """

        :param db_file: path to file with MaxMind database
        :param ip_column_name_or_expr: name of the column with IP information or SQL expression
        :param dest_column_name: name of the column in which data will be stored
        """

    def __type__for_null__(self):
        return ("struct<city:string, country:string, country_code:string, latitude:double, "
                "longitude:double, accuracy_radius:int>")

    def create_pandas_udf_function(self):
        def extract_geoip_data(ip: str, geocity, cache: Dict[str, Union[str, Dict[str, Any]]]):
            cv = cache.get(ip)
            if cv is not None:
                if isinstance(cv, str):
                    return None
                return cv
            rc: Union[str, Dict[str, Any]] = GeoIPEnrichment.EMPTY_RECORD
            if ip and is_public_ip(ip):
                try:
                    record = geocity.city(ip)
                    rc = {
                        'city': record.city.name,
                        'country': record.country.name,
                        'country_code': record.country.iso_code,
                        'latitude': record.location.latitude,
                        'longitude': record.location.longitude,
                        'accuracy_radius': record.location.accuracy_radius
                    }
                except (geoip2.errors.AddressNotFoundError, ValueError):
                    pass

            cache[ip] = rc

            return rc

        @F.pandas_udf(
            "city string, country string, country_code string, latitude double, longitude double, accuracy_radius int")
        def get_geoip_data(ips: pd.Series) -> pd.DataFrame:
            geocity = self._get_database()
            cache: Dict[str, Union[str, Dict[str, Any]]] = {}
            extracted = ips.apply(lambda ip: extract_geoip_data(ip, geocity, cache))

            return pd.DataFrame(extracted.values.tolist())

        return get_geoip_data
    
class ASNEnrichment(MaxMindEnrichmentBase):
    """Class that enriches DataFrame with data about Autonomous System (AS)
    """
    EMPTY_RECORD = {'as_number': None, 'as_org': None, 'as_network': None}

    def __init__(self, db_file: str, ip_column_name_or_expr: str, dest_column_name: str = "as_data"):
        super().__init__("ASN", db_file, ip_column_name_or_expr, dest_column_name)
        """

        :param db_file: path to file with MaxMind database
        :param ip_column_name_or_expr: name of the column with IP information or SQL expression
        :param dest_column_name: name of the column in which data will be stored
        """

    def __type__for_null__(self):
        return "struct<as_number:int, as_org:string, as_network:string>"

    def create_pandas_udf_function(self):
        def extract_asn_data(ip: str, asn, cache: Dict[str, Union[str, Dict[str, Any]]]):
            cv = cache.get(ip)
            if cv is not None:
                if isinstance(cv, str):
                    return None
                return cv
            rc: Union[str, Dict[str, Any]] = ASNEnrichment.EMPTY_RECORD
            if ip and is_public_ip(ip):
                try:
                    record = asn.asn(ip)
                    rc = {'as_number': record.autonomous_system_number,
                          'as_org': record.autonomous_system_organization,
                          'as_network': str(record.network)}
                except (geoip2.errors.AddressNotFoundError, ValueError):
                    pass

            cache[ip] = rc

            return rc

        @F.pandas_udf("as_number int, as_org string, as_network string")
        def get_asn_data(ips: pd.Series) -> pd.DataFrame:
            asn = self._get_database()
            cache: Dict[str, Union[str, Dict[str, Any]]] = {}
            extracted = ips.apply(lambda ip: extract_asn_data(ip, asn, cache))

            return pd.DataFrame(extracted.values.tolist())

        return get_asn_data


# MAGIC %md
# MAGIC ## GeoIP Database Configuration (Optional)
# MAGIC
# MAGIC GeoIP enrichment is **disabled by default**. To enable it, configure a Volume path to MaxMind GeoLite2 databases:
# MAGIC
# MAGIC **Option 1: Spark Config (Recommended - Cluster-wide)**
# MAGIC ```python
# MAGIC spark.conf.set("spark.databricks.geoip.city.path", "/Volumes/catalog/schema/geoip/GeoLite2_City.mmdb")
# MAGIC spark.conf.set("spark.databricks.geoip.asn.path", "/Volumes/catalog/schema/geoip/GeoLite2_ASN.mmdb")
# MAGIC ```
# MAGIC
# MAGIC **Option 2: Widget (Per-notebook)**
# MAGIC ```python
# MAGIC dbutils.widgets.text("geoip_city_db_path", "/Volumes/catalog/schema/geoip/GeoLite2_City.mmdb")
# MAGIC dbutils.widgets.text("geoip_asn_db_path", "/Volumes/catalog/schema/geoip/GeoLite2_ASN.mmdb")
# MAGIC ```
# MAGIC
# MAGIC **Note:** Detections will run with or without GeoIP data. If not configured, geo enrichment is simply skipped.


def get_geoip_db_path(db_type='city'):
    """
    Get GeoIP database path from Volume configuration.

    Args:
        db_type: 'city' or 'asn'

    Returns:
        Path to database file in Volume, or None if not configured
    """
    widget_name = f"geoip_{db_type}_db_path"
    config_key = f"spark.databricks.geoip.{db_type}.path"

    # Try widget first
    try:
        widget_path = dbutils.widgets.get(widget_name)
        if widget_path and widget_path.strip():
            return widget_path.strip()
    except:
        pass

    # Try Spark config
    try:
        config_path = spark.conf.get(config_key, None)
        if config_path and config_path.strip():
            return config_path.strip()
    except:
        pass

    # Not configured - disabled by default
    return None

# Initialize GeoIP enrichers (optional - disabled by default)
geo_enricher = None
asn_enricher = None
geo_info = None
asn_info = None

# Try to load GeoIP City database (optional)
city_db_path = get_geoip_db_path('city')
if city_db_path:
    try:
        geo_enricher = GeoIPEnrichment(city_db_path, ip_column_name_or_expr='geo')
        geo_info = geo_enricher.create_pandas_udf_function()
        print(f"✓ GeoIP City enrichment enabled: {city_db_path}")
    except Exception as e:
        print(f"⚠️ GeoIP City database found but failed to load: {e}")

# Try to load GeoIP ASN database (optional)
asn_db_path = get_geoip_db_path('asn')
if asn_db_path:
    try:
        asn_enricher = ASNEnrichment(asn_db_path, ip_column_name_or_expr=None)
        asn_info = asn_enricher.create_pandas_udf_function()
        print(f"✓ GeoIP ASN enrichment enabled: {asn_db_path}")
    except Exception as e:
        print(f"⚠️ GeoIP ASN database found but failed to load: {e}")

# If neither is configured, note that it's disabled (not an error)
if not city_db_path and not asn_db_path:
    print("ℹ️ GeoIP enrichment disabled (not configured)")


import functools
from enum import Enum
import uuid
from pyspark.sql.functions import col, lit, to_timestamp, to_json, struct, current_timestamp
from pyspark.sql.types import StructType, StructField, StringType, TimestampType
from delta.tables import DeltaTable

class Output(Enum):
    asDataFrame = "dataframe"
    asAlert = "alert"

alerts_schema = StructType([
    StructField("alert_id", StringType(), False),
    StructField("alertTime", TimestampType(), True),
    StructField("eventTime", TimestampType(), True),
    StructField("user_email", StringType(), True),
    StructField("event_type", StringType(), False),
    StructField("source_ip", StringType(), True),
    StructField("event_data", StringType(), True)  # JSON string
])

def _alerts_df(df):
    alerts_df = df.select(
        lit(str(uuid.uuid4())).alias("alert_id"),
        current_timestamp().alias("alertTime"),
        to_timestamp(col("EVENT_DATE")).alias("eventTime"),
        col("SRC_USER").alias("user_email"),
        col("ACTION").alias("event_type"),
        col("SRC_IP").alias("source_ip"),
        to_json(struct([col(c) for c in df.columns])).alias("event_data")
        ).na.fill({"user_email": "unknown", "source_ip": "unknown"}
    )
    
    return alerts_df

def _write_alerts(df, alerts_table_path: str):
    
    delta_table = DeltaTable.forPath(spark, alerts_table_path)
    delta_table.alias("target").merge(
        df.alias("source"),
        "target.alert_id = source.alert_id"
        ).whenNotMatchedInsertAll().execute()
    return


def detect(func=None, *, output=None, columns=None, arg=None):
    """Decorator function with arguments
    Decorator can be used with or without arguments
    Examples:
        >>>
        >>> @detect
        >>> def func():
        >>>     pass
        >>>
        >>> @detect(arg='foo')
        >>> def func():
        >>>     pass
        >>>
    """
    # 1. Decorator arguments are applied to itself as partial arguments
    if func is None:
        return functools.partial(detect, output=output, columns=columns, arg=arg)

    # 2. logic with the arguments
 
    # 3. Handles the actual decorating
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        # Write decorator function logic here
        # Before function call
        # ...
        result = func(*args, **kwargs)
        # After function call
        # ...
        if output == Output.asAlert:
            result = _alerts_df(result)
    

        return result
    return wrapper

from datetime import datetime, timedelta, timezone

def get_time_range_from_widgets(default_hours: int = 24):
    from datetime import datetime, timedelta, timezone

    now_utc = datetime.now(timezone.utc)
    default_latest = now_utc.strftime("%Y-%m-%d %H:%M:%S")
    default_earliest = (now_utc - timedelta(hours=default_hours)).strftime("%Y-%m-%d %H:%M:%S")

    dbutils.widgets.text("earliest", default_earliest)
    dbutils.widgets.text("latest", default_latest)

    earliest = dbutils.widgets.get("earliest")
    latest = dbutils.widgets.get("latest")

    print(f"✅ Using time range: earliest = {earliest}, latest = {latest}")
    return earliest, latest

def get_notebook_path():
    """Get the current notebook's workspace path. Works on both classic compute and serverless."""
    return dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()

def get_detections_dir():
    """Get the detections directory path relative to the current notebook."""
    import os
    notebook_path = get_notebook_path()
    # notebook_path is like: /Users/user@email.com/repo/base/notebooks/notebook_name
    notebooks_dir = os.path.dirname(notebook_path)  # /Users/user@email.com/repo/base/notebooks
    base_dir = os.path.dirname(notebooks_dir)       # /Users/user@email.com/repo/base
    return os.path.join(base_dir, "detections")     # /Users/user@email.com/repo/base/detections

def run_all_detections(
    workspace_dir: str = None,
    earliest: str = None,
    latest: str = None,
    notebook_filter: str = None
):
    import re
    from databricks.sdk import WorkspaceClient

    # If workspace_dir not provided, derive it from the current notebook path
    if workspace_dir is None:
        workspace_dir = get_detections_dir()
    elif workspace_dir.startswith("..") or workspace_dir.startswith("/Workspace"):
        # Convert relative or /Workspace paths to workspace API paths
        if workspace_dir.startswith("/Workspace"):
            workspace_dir = workspace_dir.replace("/Workspace", "", 1)
        else:
            # Relative path - resolve from current notebook
            import os
            notebook_path = get_notebook_path()
            notebooks_dir = os.path.dirname(notebook_path)
            workspace_dir = os.path.normpath(os.path.join(notebooks_dir, workspace_dir))

    print(f"Scanning Workspace notebooks in: {workspace_dir}")

    try:
        w = WorkspaceClient()

        # Scan both binary and behavioral subdirectories
        notebook_paths = []
        for subdir in ["binary", "behavioral"]:
            subdir_path = f"{workspace_dir}/{subdir}"
            try:
                notebooks = list(w.workspace.list(subdir_path))
                paths = [
                    obj.path for obj in notebooks
                    if obj.path and (obj.path.endswith(".py") or obj.path.endswith(".ipynb") or
                                   obj.object_type and obj.object_type.name == "NOTEBOOK")
                ]
                notebook_paths.extend(paths)
                print(f"  Found {len(paths)} detections in {subdir}/")
            except Exception as e:
                print(f"  Warning: Failed to scan {subdir_path}: {e}")

        print(f"Total detections found: {len(notebook_paths)}")

    except Exception as e:
        print(f"Failed to scan detections: {e}")
        return

    if notebook_filter:
        pattern = re.compile(notebook_filter)
        notebook_paths = [n for n in notebook_paths if pattern.search(n)]

    for full_path in notebook_paths:
        # Remove file extension for notebook.run
        run_path = re.sub(r'\.(py|ipynb)$', '', full_path)

        try:
            dbutils.notebook.run(run_path, 3600, arguments={
                "earliest": earliest,
                "latest": latest
            })

        except Exception as e:
            print(f"Failed to run {run_path}: {e}")
